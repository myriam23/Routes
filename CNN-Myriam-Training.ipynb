{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import tensorflow as tf\n",
    "import os,sys\n",
    "from PIL import Image\n",
    "import albumentations as albu\n",
    "import cv2\n",
    "import helper\n",
    "import pandas as pd \n",
    "import h5py\n",
    "import skimage.color as color\n",
    "import augmentation as augment\n",
    "import models as mod\n",
    "import keras\n",
    "import submission as sub\n",
    "import math as math\n",
    "from keras import datasets, layers, models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 100 images\n",
      "Loading 100 images\n"
     ]
    }
   ],
   "source": [
    "# Loaded a set of images\n",
    "root_dir = \"Datasets/training/\"\n",
    "\n",
    "n = 100\n",
    "\n",
    "imgs = []\n",
    "gt_imgs = []\n",
    "\n",
    "image_dir = root_dir + \"images/\"\n",
    "files = os.listdir(image_dir)\n",
    "print(\"Loading \" + str(n) + \" images\")\n",
    "imgs = [helper.load_image(image_dir + files[i]) for i in range(n)]\n",
    "\n",
    "gt_dir = root_dir + \"groundtruth/\"\n",
    "print(\"Loading \" + str(n) + \" images\")\n",
    "gt_imgs = [helper.load_image(gt_dir + files[i]) for i in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "n = 50\n",
    "patch_size = 16\n",
    "\n",
    "img_test = []\n",
    "\n",
    "for i in range(1, 51):\n",
    "    image_filename = 'Datasets/test_set_images/test_' + str(i) + '/test_' + str(i) + '.png' \n",
    "    img_test.append(helper.load_image(image_filename))\n",
    "\n",
    "img_patches_test = [helper.img_crop(img_test[i], patch_size, patch_size) for i in range(n)]\n",
    "img_patches_test = np.asarray([img_patches_test[i][j] for i in range(len(img_patches_test)) for j in range(len(img_patches_test[i]))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Types of augmentation\"\n",
    "#light & medium need uint8 imgs ! \n",
    "light = albu.Compose([albu.RandomBrightnessContrast(p=1),albu.RandomGamma(p=1), albu.CLAHE(p=1)], p=1)\n",
    "\n",
    "medium = albu.Compose([albu.CLAHE(p=1), albu.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=50, val_shift_limit=50, p=1)], p=1)\n",
    "\n",
    "strong = albu.Compose([albu.ChannelShuffle(p=1)], p=1)\n",
    "    \n",
    "grid_shuffle = albu.RandomGridShuffle(grid=(5, 5), always_apply=False, p=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_patches, gt_patches = augment.extract_patches(imgs, gt_imgs)\n",
    "y = to_categorical(gt_patches) #for categorical_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rotation set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rot, gt_rot = augment.rotation(imgs, gt_imgs, rg = 55)\n",
    "imgrot_patches, gtrot_patches = augment.extract_patches(img_rot, gt_rot)\n",
    "yrot = to_categorical(gtrot_patches) #for categorical_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### elastic set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_elastic, gt_elastic = augment.elastic(imgs, gt_imgs)\n",
    "imgelast_patches, gtelast_patches = augment.extract_patches(img_elastic, gt_elastic)\n",
    "yelastic = to_categorical(gtelast_patches) #for categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_elasticNorm = gt_imgs + gt_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_pat_elastnorm, GT_pat_norm = augment.extract_patches(img_elastic,gt_elasticNorm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### grid shuffle set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shuffle, gt_shuffle = augment.transform(grid_shuffle, imgs, gt_imgs, convert = False)\n",
    "imgshuffle_patches, gtshuffle_patches = augment.extract_patches(img_shuffle, gt_shuffle)\n",
    "yshuffle = to_categorical(gtshuffle_patches) #for categorical_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All adds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_all = imgs + img_rot[-100:] + img_elastic[-100:] + img_shuffle[-100:] + img_elastic[-100:]\n",
    "gt_all = gt_imgs + gt_rot[-100:] + gt_elastic[-100:] + gt_shuffle[-100:] + gt_imgs\n",
    "\n",
    "imgsall_patches, gtall_patches = augment.extract_patches(imgs_all, gt_all)\n",
    "yall = to_categorical(gtall_patches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patcher 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 16 # each patch is 16*16 pixels\n",
    "window_size=100\n",
    "img3000_patches = [augment.patcher_3000(imgs[i], patch_size, window_size) for i in range(len(imgs))]\n",
    "gt_patches = [helper.img_crop(gt_imgs[i], patch_size, patch_size) for i in range(len(gt_imgs))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linearize list of patches\n",
    "img3000_patches = np.asarray([img3000_patches[i][j] for i in range(len(img3000_patches)) for j in range(len(img3000_patches[i]))])\n",
    "gt_patches =  np.asarray([gt_patches[i][j] for i in range(len(gt_patches)) for j in range(len(gt_patches[i]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_patches = helper.patches_labelization(gt_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempimg_rot_elastic = img_rot + img_elastic[-100:]\n",
    "tempgt_rot_elastic = gt_rot + gt_elastic[-100:]\n",
    "\n",
    "img3000_rot_elastic_patches = [augment.patcher_3000(tempimg_rot_elastic[i], patch_size, window_size) for i in range(len(tempimg_rot_elastic))]\n",
    "gt3000_rot_elastic_patches = [helper.img_crop(tempgt_rot_elastic[i], patch_size, patch_size) for i in range(len(tempgt_rot_elastic))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linearize list of patches\n",
    "img_rot_elastic_3000_patches = np.asarray([img3000_rot_elastic_patches[i][j] for i in range(len(img3000_rot_elastic_patches)) for j in range(len(img3000_rot_elastic_patches[i]))])\n",
    "gt_rot_elastic_3000_patches =  np.asarray([gt_rot_elastic_3000_patches[i][j] for i in range(len(gt_rot_elastic_3000_patches)) for j in range(len(gt_rot_elastic_3000_patches[i]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_rot_elastic_3000_patches = helper.patches_labelization(gt_rot_elastic_3000_patches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typesbin = ['SoftmaxBinary','SigmoidBinary']\n",
    "typescat = ['SoftmaxCategorical', 'SigmoidCategorical']\n",
    "#data = ['rotated','elastic','shuffled','all', 'elasticNorm', 'patch3000', 'patch3000+rot+elastic']\n",
    "#databinary = [[imgrot_patches,gtrot_patches],[imgelast_patches,gtelast_patches],[imgshuffle_patches,gtshuffle_patches], [imgsall_patches, gtall_patches], [IMG_patelastNorm, GT_pat_norm], [img3000_patches, gt_patches], [img_rot_elastic_3000_patches, gt_rot_elastic_3000_patches]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model1Cat = pd.DataFrame()\n",
    "batch_size = 64 \n",
    "e = 15\n",
    "\n",
    "for i,typ in enumerate(typescat): \n",
    "    M1 = mod.model1(typ)\n",
    "    M1.compile(loss = 'categorical_crossentropy', optimizer = keras.optimizers.Adam(lr = 1e-3), metrics = ['accuracy'])\n",
    "    for j, dat in enumerate(datacateg): \n",
    "        m1Hist = M1.fit(dat[0],dat[1],epochs=e, validation_split = 0.2)\n",
    "        model1Cat.append(m1Hist.history['val_accuracy'])\n",
    "        modelpath = 'saved_M1_fit_{0}_{1}_{2}.h5'.format(typ,data[j], e)\n",
    "        M1.save(modelpath)        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model1Bin = pd.DataFrame()\n",
    "batch_size = 64 \n",
    "e = 15\n",
    "for i,typ in enumerate(typesbin): \n",
    "    M1 = mod.model1(typ)\n",
    "    M1.compile(loss = 'binary_crossentropy', optimizer = keras.optimizers.Adam(lr = 1e-3), metrics = ['accuracy', f1_m])\n",
    "    for j, dat in enumerate(databinary): \n",
    "        m1Hist = M1.fit(dat[0],dat[1], epochs=e, validation_split = 0.2)\n",
    "        model1Bin.append(m1Hist.history['val_accuracy'])\n",
    "        modelpath = 'saved_M1_fit_{0}_{1}_{2}.h5'.format(typ,data[j], e)\n",
    "        M1.save(modelpath)   \n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model2Cat = pd.DataFrame()\n",
    "batch_size = 64 \n",
    "e = 15\n",
    "\n",
    "for i,typ in enumerate(typescat): \n",
    "    M2 = mod.model2(typ)\n",
    "    M2.compile(loss = 'categorical_crossentropy', optimizer = keras.optimizers.Adam(lr = 1e-3), metrics = ['accuracy'])\n",
    "    for j, dat in enumerate(datacateg): \n",
    "        m2Hist = M2.fit(dat[0],dat[1],epochs=e, validation_split = 0.2)\n",
    "        model2Cat.append(m2Hist.history['val_accuracy'])\n",
    "        modelpath = 'saved_M2_fit_{0}_{1}_{2}.h5'.format(typ,data[j], e)\n",
    "        M2.save(modelpath) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model2Bin = pd.DataFrame()\n",
    "batch_size = 64 \n",
    "e = 15\n",
    "\n",
    "for i,typ in enumerate(typesbin): \n",
    "    M2 = mod.model2(typ)\n",
    "    M2.compile(loss = 'binary_crossentropy', optimizer = keras.optimizers.Adam(lr = 1e-3), metrics = ['accuracy', helper.f1_m])\n",
    "    for j, dat in enumerate(databinary): \n",
    "        m2Hist = M2.fit(dat[0],dat[1], epochs=e, validation_split = 0.2)\n",
    "        model2Bin.append(m2Hist.history['val_accuracy'])  \n",
    "        modelpath = 'saved_M2_fit_{0}_{1}_{2}.h5'.format(typ,data[j], e)\n",
    "        M2.save(modelpath) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "4883/4883 [==============================] - 647s 132ms/step - loss: 0.4672 - accuracy: 0.7683\n",
      "Epoch 2/30\n",
      "4883/4883 [==============================] - 532s 109ms/step - loss: 0.3671 - accuracy: 0.8338\n",
      "Epoch 3/30\n",
      "1306/4883 [=======>......................] - ETA: 7:02 - loss: 0.3299 - accuracy: 0.8556"
     ]
    }
   ],
   "source": [
    "model2CatGen = pd.DataFrame()\n",
    "batch = 64 \n",
    "window = 16\n",
    "e = 30\n",
    "databinary = [ [imgsall_patches, gtall_patches],[imgrot_patches, gtrot_patches],[imgshuffle_patches,gtshuffle_patches],  [IMG_pat_elastnorm, GT_pat_norm]]\n",
    "data = ['all','rotated','shuffled', 'elasticNorm'] \n",
    "\n",
    "M2 = mod.model2('SoftmaxCategorical')\n",
    "M2.compile(loss = 'categorical_crossentropy', optimizer = keras.optimizers.Adam(lr = 1e-3), metrics = ['accuracy'])\n",
    "for j, dat in enumerate(databinary): \n",
    "    earlystop = keras.callbacks.EarlyStopping(monitor='accuracy', min_delta=0.01, patience=5, verbose=0, mode='auto', baseline=None, restore_best_weights=True)\n",
    "    \n",
    "    m2Hist = M2.fit_generator(mod.generate_minibatch(dat[0],dat[1],batch, window), epochs=e, steps_per_epoch = math.ceil(dat[0].shape[0] / batch), callbacks = [earlystop] )\n",
    "    model2CatGen.append(m2Hist.history['accuracy'])  \n",
    "    modelpath = 'saved_M2_Gen_SoftmaxCategorical_{0}_{1}.h5'.format(data[j], e)\n",
    "    M2.save(modelpath) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data3000 = ['patch3000', 'patch3000+rot+elastic']\n",
    "#databin3000 = [[img3000_patches, gt_patches], [img_rot_elastic_3000_patches, gt_rot_elastic_3000_patches]]\n",
    "\n",
    "window = 100\n",
    "data3000 = ['patch3000']\n",
    "databin3000 = [img3000_patches, gt_patches]\n",
    "model2CatGen3000 = pd.DataFrame()\n",
    "batch = 64 \n",
    "e = 30\n",
    "shape = (100,100,3)\n",
    "\n",
    "M2 = mod.model2('SoftmaxCategorical', shape)\n",
    "M2.compile(loss = 'categorical_crossentropy', optimizer = keras.optimizers.Adam(lr = 1e-3), metrics = ['accuracy'])\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='accuracy', min_delta=0.01, patience=5, verbose=0, mode='auto', baseline=None, restore_best_weights=True)\n",
    "    \n",
    "m2Hist = M2.fit_generator(mod.generate_minibatch(img3000_patches,gt_patches,batch, window), epochs=e, steps_per_epoch = math.ceil(img3000_patches.shape[0] / batch), callbacks = [earlystop] )\n",
    "model2CatGen3000.append(m2Hist.history['accuracy'])  \n",
    "modelpath = 'saved_M2_Gen_SoftmaxCategorical_3000_{0}.h5'.format(e)\n",
    "M2.save(modelpath) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model3Cat = pd.DataFrame()\n",
    "batch_size = 64 \n",
    "e = 15\n",
    "\n",
    "for i,typ in enumerate(typescat): \n",
    "    M3 = mod.model3(typ)\n",
    "    M3.compile(loss = 'categorical_crossentropy', optimizer = keras.optimizers.Adam(lr = 1e-3), metrics = ['accuracy'])\n",
    "    for j, dat in enumerate(datacateg): \n",
    "        m3Hist = M3.fit(datcat[0],datcat[1],epochs=e, validation_split = 0.2)\n",
    "        model3Cat.append(m3Hist.history['val_accuracy'])\n",
    "        modelpath = 'saved_M3_fit_{0}_{1}_{2}.h5'.format(typ,data[j], e)\n",
    "        M3.save(modelpath)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model3Bin = pd.DataFrame()\n",
    "batch_size = 64 \n",
    "e = 15\n",
    "\n",
    "for i,typ in enumerate(typesbin): \n",
    "    M3 = mod.model3(typ)\n",
    "    M3.compile(loss = 'binary_crossentropy', optimizer = keras.optimizers.Adam(lr = 1e-3), metrics = ['accuracy', helper.f1_m])\n",
    "    for j, dat in enumerate(databinary):\n",
    "        m3Hist = M3.fit(dat[0],dat[1], epochs=e, validation_split = 0.2)\n",
    "        model3Bin.append(m3Hist.history['val_accuracy'])  \n",
    "        modelpath = 'saved_M3_fit_{0}_{1}_{2}.h5'.format(typ,data[j], e)\n",
    "        M3.save(modelpath)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3CatGen = pd.DataFrame()\n",
    "batch_size = 64 \n",
    "e = 15\n",
    "\n",
    "M3 = mod.model3('SoftmaxCategorical')\n",
    "M3.compile(loss = 'categorical_crossentropy', optimizer = keras.optimizers.Adam(lr = 1e-3), metrics = ['accuracy'])\n",
    "M3.summary()\n",
    "for j, dat in enumerate(databinary): \n",
    "    m3Hist = M3.fit_generator(mod.generate_minibatch(dat[0],dat[1],batch_size), epochs=e, steps_per_epoch = math.ceil(dat[0].shape[0] / batch_size) )\n",
    "    model3CatGen.append(m3Hist.history['accuracy'])  \n",
    "    modelpath = 'saved_M3_Gen_SoftmaxCategorical_{0}_{1}.h5'.format(data[j], e)\n",
    "    M3.save(modelpath) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = [img3000_patches, gt_patches]\n",
    "\n",
    "model3CatGen3000 = pd.DataFrame()\n",
    "batch_size = 64 \n",
    "e = 25\n",
    "\n",
    "M3 = mod.model3('SoftmaxCategorical', (100,100,3))\n",
    "M3.compile(loss = 'categorical_crossentropy', optimizer = keras.optimizers.Adam(lr = 1e-3), metrics = ['accuracy'])\n",
    "m3Hist = M3.fit_generator(mod.generate_minibatch(dat[0],dat[1],batch_size), epochs=e, steps_per_epoch = math.ceil(dat[0].shape[0] / batch_size) )\n",
    "model3CatGen3000.append(m3Hist.history['accuracy'])  \n",
    "modelpath = 'saved_M3_Gen_SoftmaxCategorical_3000rotelastic_{0}.h5'.format(e)\n",
    "M3.save(modelpath) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model4CatGen = pd.DataFrame()\n",
    "batch_size = 64 \n",
    "e = 15\n",
    "step = math.ceil(imgsall_patches.shape[0] / batch_size) \n",
    "fit_gen = databinary.copy()\n",
    "\n",
    "for i,typ in enumerate(typescat): \n",
    "    M4 = mod.model4(typ)\n",
    "    M4.compile(loss = 'categorical_crossentropy', optimizer = keras.optimizers.Adam(lr = 1e-3), metrics = ['accuracy'])\n",
    "    for j, dat in enumerate(fit_gen): \n",
    "        m4Hist = M4.fit_generator(mod.generate_minibatch(dat[0],dat[1], batch_size), epochs=e, steps_per_epoch= step)\n",
    "        model5CatGen.append(m4Hist.history['accuracy'])\n",
    "        modelpath = 'saved_M4_Gen_{0}_{1}_{2}.h5'.format(typ,data[j], e)\n",
    "        M4.save(modelpath)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model4BinGen = pd.DataFrame()\n",
    "batch_size = 64 \n",
    "e = 15\n",
    "\n",
    "for i,typ in enumerate(typesbin): \n",
    "    M4 = mod.model4(typ)\n",
    "    M4.compile(loss = 'binary_crossentropy', optimizer = keras.optimizers.Adam(lr = 1e-3), metrics = ['accuracy'])\n",
    "    for j, dat in enumerate(fit_gen): \n",
    "        step = math.ceil(dat[0].shape[0] / batch_size) \n",
    "        m4Hist = M4.fit_generator(mod.generate_minibatch(dat[0],dat[1], batch_size), epochs=e, steps_per_epoch= step)\n",
    "        model4BinGen.append(m4Hist.history['accuracy'])\n",
    "        modelpath = 'saved_M4_Gen_{0}_{1}_{2}.h5'.format(typ,data[j], e)\n",
    "        M4.save(modelpath)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4CatGen = pd.DataFrame()\n",
    "batch_size = 64 \n",
    "e = 15\n",
    "\n",
    "M4 = mod.model4('SoftmaxCategorical')\n",
    "M4.compile(loss = 'categorical_crossentropy', optimizer = keras.optimizers.Adam(lr = 1e-3), metrics = ['accuracy'])\n",
    "for j, dat in enumerate(databinary): \n",
    "    m4Hist = M4.fit_generator(mod.generate_minibatch(dat[0],dat[1],batch_size), epochs=e, steps_per_epoch = math.ceil(dat[0].shape[0] / batch_size) )\n",
    "    model4CatGen.append(m4Hist.history['accuracy'])  \n",
    "    modelpath = 'saved_M4_Gen_SoftmaxCategorical_{0}_{1}.h5'.format(data[j], e)\n",
    "    M4.save(modelpath) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3000 = ['patch3000', 'patch3000+rot+elastic']\n",
    "databin3000 = [[img3000_patches, gt_patches], [img_rot_elastic_3000_patches, gt_rot_elastic_3000_patches]]\n",
    "window = 100\n",
    "model4CatGen3000 = pd.DataFrame()\n",
    "batch = 64 \n",
    "e = 2\n",
    "shape = (100,100,3)\n",
    "\n",
    "M4 = mod.model4('SoftmaxCategorical', shape)\n",
    "M4.compile(loss = 'categorical_crossentropy', optimizer = keras.optimizers.Adam(lr = 1e-3), metrics = ['accuracy'])\n",
    "for j, dat in enumerate(databin3000): \n",
    "    earlystop = keras.callbacks.EarlyStopping(monitor='accuracy', min_delta=0.01, patience=5, verbose=0, mode='auto', baseline=None, restore_best_weights=True)\n",
    "    \n",
    "    m4Hist = M4.fit_generator(mod.generate_minibatch(dat[0],dat[1],batch, window), epochs=e, steps_per_epoch = math.ceil(dat[0].shape[0] / batch), callbacks = [earlystop] )\n",
    "    model4CatGen3000.append(m4Hist.history['accuracy'])  \n",
    "    modelpath = 'saved_M4_Gen_SoftmaxCategorical_{0}_{1}.h5'.format(data[j], e)\n",
    "    M4.save(modelpath) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLOTTING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
